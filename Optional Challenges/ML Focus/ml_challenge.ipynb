{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Challenge (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, test, optimize, and analyze the performance of a classification model using a methodology of your choice for the randomly generated moons dataset.\n",
    "\n",
    "You are not being evaluated for the performance of your model. Instead, we are interested in whether you can implement a simple but rigorous ML workflow.\n",
    "\n",
    "Show all of your work in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you are free to use any package you deem fit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, Y = make_moons(random_state=42, n_samples=(50, 450), noise=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Testing Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=50)\n",
    "base_model = RandomForestClassifier(n_estimators=100,\n",
    "                                    max_depth=None,\n",
    "                                    min_samples_split=2,\n",
    "                                    min_samples_leaf=1,\n",
    "                                    random_state = 2)\n",
    "base_model.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred_base = base_model.predict(X_test)\n",
    "accuracy_base = accuracy_score(Y_test, Y_pred_base)\n",
    "print(f'Base Model Testing Accuracy: {accuracy_base}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing / Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Model Testing Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "#Grid search approach\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=base_model, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "best_params = grid_search.best_params_\n",
    "grid_model= RandomForestClassifier(n_estimators = best_params['n_estimators'], \n",
    "                                    max_depth = best_params['max_depth'],\n",
    "                                    min_samples_split = best_params['min_samples_split'],\n",
    "                                    min_samples_leaf = best_params['min_samples_split'],\n",
    "                                    random_state = 2 )\n",
    "grid_model.fit(X_train, Y_train)\n",
    "Y_pred_grid = grid_model.predict(X_test)\n",
    "accuracy_grid = accuracy_score(Y_test, Y_pred_grid)\n",
    "print(f'Grid Model Testing Accuracy: {accuracy_grid}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Training Accuracy: 1.0\n",
      "Grid Model Training Accuracy: 0.9675\n"
     ]
    }
   ],
   "source": [
    "accuracy_base_train = accuracy_score(Y_train, base_model.predict(X_train))\n",
    "print(f'Base Model Training Accuracy: {accuracy_base_train}')\n",
    "\n",
    "accuracy_grid_train = accuracy_score(Y_train, grid_model.predict(X_train))\n",
    "print(f'Grid Model Training Accuracy: {accuracy_grid_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation that the accuracy score increased on the test set while decreasing on the training set suggests that the initial model was likely overfitting the training data. The grid search appears to have identified hyperparameters that make the model more general, enabling it to perform better on new, unseen data. This trade-off often indicates a well-tuned model that is likely to generalize better to future data, even if it performs slightly worse on the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e78b6b4158d8f577a77be3bef6c4f5889b406541923fa59adc2e6c48950512fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
